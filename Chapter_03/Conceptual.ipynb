{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Linear Regression - Conceptual Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1:\n",
    "\n",
    "Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\n",
    "\n",
    "### Answer:\n",
    "\n",
    "Each null hypothesis for every variable TV, Radio and Newspaper is interpreted to whether the coefficient of the predictor is not significantly different from zero.\n",
    "\n",
    "1. TV (p-value<0.0001): based on the p-value we can reject the null hypothesis and conclude that spending 1000$ on TV advertisement increases Sale for 460$ while holding other variables fixed.\n",
    "\n",
    "2. Radio (p-value<0.0001): based on the p-value we can reject the null hypothesis and conclude that spending 1000$ on Radio advertisement increases Sale for 189$ while holding other variables fixed.\n",
    "\n",
    "3. Newspaper (p-value=0.8599): based on the p-value we accept the null hypothesis and conclude that Newspaper advertisement has no effect on Sale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2:\n",
    "\n",
    "Carefully explain the differences between the KNN classifier and KNN regression methods.\n",
    "\n",
    "### Answer:\n",
    "\n",
    "KNN Classifier: For a given K number, K Nearest Neighbors (KNN) classifier identifies the K nearest points in training data to estimate a test observation category. It estimates the conditional probability of each class and categorize the observation in the highest probability category.\n",
    "\n",
    "KNN Regression: For a given K number, KNN regression estimates a test observation by averaging the values of all the K nearest points to the test observation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3:\n",
    "\n",
    "Suppose we have a data set with five predictors, X1 = GPA, X2 = IQ, X3 = Level (1 for College and 0 for High School), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get β0 = 50, β1 = 20, β2 = 0.07, β3 = 35, β4 = 0.01, β5 = −10.\n",
    "\n",
    "### a) Which answer is correct, and why?\n",
    "\n",
    "1. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\n",
    "\n",
    "2. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\n",
    "\n",
    "3. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\n",
    "\n",
    "4. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer a:\n",
    "\n",
    "1. False: Considering values of beta3(35) and beta5(-10), the starting salary of college and high school graduate most of the times are lower than college graduates except for college graduate who have higher GPA than 3.5.\n",
    "\n",
    "2. True: Considering values of beta3(35) and beta5(-10), the starting salary of college and high school are higher than high school graduates, however, college graduates who have GPA greater than 3.5 start their salary lower than high school graduates.\n",
    "\n",
    "3. True: With considering the minus coefficient value of interaction term between GPA and Level, high GPA (greater than 3.5) will estimate lower value of Starting Salary for college graduates.\n",
    "\n",
    "4. False: With considering the minus coefficient value of interaction term between GPA and Level, high GPA (greater than 3.5) will estimate lower value of Starting Salary for college graduates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n",
    "\n",
    "#### Answer b:\n",
    "\n",
    "salary = 50 + (4x20) + (110x0.07) + (35x1) + (0.01x4x110) + (-10x1x4) = 137.1\n",
    "\n",
    "### c) rue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n",
    "\n",
    "### Answer c:\n",
    "\n",
    "False, the coefficient of GPA and IQ defines the effect when the GPA and IQ is both high and since this values is low we can interpret that for graduates having high GPA and high IQ will have a little synergy effect on their starting salary. The level of evidence for coefficients is related their t-statistics and p-value.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4:\n",
    "\n",
    "I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 +β1X +β2X^2 +β3X^3 +ε.\n",
    "\n",
    "(a) Suppose that the true relationship between X and Y is linear, i.e. Y = β0 + β1X + ε. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Although, the true relationship between X and Y is linear, mostly we expect the cubic regressions to fit the training data better for more their more flexibility and wiggly shape (high variance). So we expect the cubic regression to have lower RSS than linear regression.\n",
    "\n",
    "b) Answer (a) using test rather than training RSS.\n",
    "\n",
    "Answer:\n",
    "\n",
    "The true f(x) is linear and linear model will work better in test observations, hence it has lower test RSS compared to cubic regression.\n",
    "\n",
    "(c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "Answer:\n",
    "\n",
    "The relationship between X and Y in not linear, hence the polynomial model can estimate the f(x) more properly and the training RSS will be lower than linear model.\n",
    "\n",
    "(d) Answer (c) using test rather than training RSS.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Considering non-linear relationship, cubic regression will preform also better on test observations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5:\n",
    "\n",
    "Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form\n",
    "\n",
    "\n",
    "![alt text](<Screenshot 2024-08-02 at 16.38.22.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "![alt text](<Screenshot 2024-08-02 at 17.04.34.png>)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.6:\n",
    "\n",
    "Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point (x ̄, y ̄).\n",
    "\n",
    "Answer:\n",
    "\n",
    "![alt text](<Screenshot 2024-08-02 at 17.20.41.png>)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.6:\n",
    "\n",
    "7. It is claimed in the text that in the case of simple linear regression of Y onto X, the R2 statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that x ̄ = y ̄ = 0.\n",
    "\n",
    "### Answer:\n",
    "\n",
    "let's find beta_zero, beta_one :\n",
    "\n",
    "![alt text](<Screenshot 2024-08-04 at 21.29.57.png>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then TSS:\n",
    "\n",
    "![alt text](<Screenshot 2024-08-04 at 21.22.13.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that x_bar and y_bar are zero, let's find the r^2:\n",
    "\n",
    "![alt text](<Screenshot 2024-08-04 at 21.39.46.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let go for R^2:\n",
    "\n",
    "![alt text](<Screenshot 2024-08-04 at 22.17.54.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that beta_1 is a constant so we can bring it out of sigma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
